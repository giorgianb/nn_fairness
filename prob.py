from itertools import product
from scipy.integrate import quad
import numpy as np
from itertools import chain
from collections import defaultdict
from abc import ABC, abstractmethod
from sklearn.neighbors import KernelDensity
from sklearn.linear_model import LinearRegression

from compute_volume import lawrence_integrate_linear_form, lawrence_integrate_linear_exp, dd_extreme
from nnenum.lpinstance import LpInstance
import swiglpk as glpk
from polytope import reduce, Polytope


class PolyFitKernelPDF:
    """Fits a polynomial to the results generated by a kernel"""
    # TODO: add "given", which allows us to predicate on some variable when predicting the 
    # continuous features
    def __init__(self, X, input_class, n_generated_features=10, deg=5, bandwidth=0.1, kernel="gaussian"):
        self.kernel = kernel
        # TODO: check whether we still need this assumption, and if so, ensure it is enforced
        # Assumption: One-Hot indices are contiguous in the array for a one-hot feature
        matches_given = np.apply_along_axis(lambda x: x in input_class, 1, X)
        if np.sum(matches_given) == 0:
            raise ValueError("No instance of class found.")

        X = X[matches_given]

        continuous_bounds = []
        spec = input_class.specification
        for label, index in spec.continuous_indices.items():
            lb, ub = spec.variables[label].min, spec.variables[label].max
            continuous_bounds.append((index, lb, ub))

        self.continuous_bounds = [[bound[1:]] for bound in sorted(continuous_bounds)]

        self.discrete_indices = tuple(sorted(spec.discrete_indices.values()))
        self.continuous_indices = tuple(sorted(spec.continuous_indices.values()))
        self.one_hot_indices = tuple(sorted(spec.one_hot_indices.values()))

        self.discrete = [make_discrete_distribution(X[:, i]) for i in self.discrete_indices]
        self.one_hot = [make_one_hot_distribution(X[:, index_group]) for index_group in self.one_hot_indices]

        self.discrete_bounds = [[(k, k) for k in sorted(d.keys())] for d in self.discrete]
        self.one_hot_bounds = [[(one_hot(k, len(d.keys())), one_hot(k, len(d.keys()))) for k in sorted(d.keys())] for d in self.one_hot]


        self.feature_generator = np.random.normal(size=(deg, n_generated_features, len(self.continuous_indices)))
        self.bandwidth = bandwidth
        self.coeffs, self.bias = self._fit(X)

        
        self.discrete_funcs = [make_discrete_func(d) for d in self.discrete]
        self.one_hot_funcs = [make_one_hot_func(d) for d in self.one_hot]

        # Note we can probably do this analytically
        self.continuous_volume = self._continuous_integrate([b[0] for b in self.continuous_bounds])
        self.discrete_volumes = [sum(f(k) for k in d.keys()) for f, d in zip(self.discrete_funcs, self.discrete)]
        self.one_hot_volumes = [sum(f(k) for k in d.keys()) for f, d in zip(self.one_hot_funcs, self.one_hot)]

        regions = sorted(
                chain(
                    zip([[k] for k in self.continuous_indices], self.continuous_bounds),
                    zip([[k] for k in self.discrete_indices], self.discrete_bounds),
                    zip(self.one_hot_indices, self.one_hot_bounds)
                    )

        )
        self._regions = tuple(map(lambda x: x[1], regions))

    def _fit(self, X):
        # Use the kernel to generate the "ground truth" pdf
        X = X[:, self.continuous_indices]
        self.kern = KernelDensity(kernel=self.kernel, bandwidth=self.bandwidth).fit(X)
        X_test = np.concatenate([np.random.uniform(low=b[0][0], high=b[0][1], size=(2*X.shape[0], 1)) for b in self.continuous_bounds], axis=-1)
        X_full = np.concatenate((X, X_test), axis=0)
        y = np.exp(self.kern.score_samples(X_full))

        # Now we generate the features
        X_train = np.einsum('dfi,bi->bdf', self.feature_generator, X_full)
        deg = np.arange(self.feature_generator.shape[0])[None, :, None] + 1
        X_train = X_train**deg
        X_train = X_train.reshape(X_train.shape[0], -1)

        lm = LinearRegression().fit(X_train, y)
        return lm.coef_.reshape(self.feature_generator.shape[:-1]), lm.intercept_

    def _continuous_sample(self, x):
        X = np.einsum('dfi,...i->...df', self.feature_generator, x)
        X = X**(np.arange(self.feature_generator.shape[0])[None, :, None] + 1).reshape(X.shape[0], -1)
        y = np.einsum('...df,df->...', X, self.coeffs) + self.bias
        return y/self.continuous_volume

    def sample(self, *args):
        """get probability density at a point"""

        x = np.array(args)
        p = self._continuous_sample(x[list(self.continuous_indices)])
        for func, volume, index in zip(self.discrete_funcs, self.discrete_volumes, self.discrete_indices):
            p *= func(x[index])/volume

        for func, volume, index_group in zip(self.one_hot_funcs, self.one_hot_volumes, self.one_hot_indices):
            p *= func(x[index_group])/volume

        return p

    @property
    def regions(self):
        return product(*self._regions)

    def _continuous_integrate(self, bounds):
        n = len(bounds)
        A = np.concatenate([np.eye(n), -np.eye(n)], axis=0)
        b = np.concatenate([np.array([b[1] for b in bounds]), -np.array([b[0] for b in bounds])], axis=0)

        poly = reduce(Polytope(A, b))
        poly.minrep = True


        lin_forms = self.feature_generator.reshape(-1, self.feature_generator.shape[-1])
        lin_forms = np.concatenate([lin_forms, np.ones((1, lin_forms.shape[1]))], axis=0)
        P = (np.arange(self.feature_generator.shape[0])[:, None].repeat(self.feature_generator.shape[1], axis=0).flatten() + 1)
        P = np.concatenate([P, np.zeros(1)], axis=0)
        vertices, active_constraints = dd_extreme(poly)

        v = lawrence_integrate_linear_form(poly, vertices, active_constraints, coeffs=lin_forms, P=P)
        coeffs = np.concatenate([self.coeffs.flatten(), self.bias*np.ones(1)], axis=-1)

        return np.sum(v*coeffs)


    def integrate(self, lpi, fixed_indices):
        # Fixed indices are those that are fixed in the input set
        # So they are not in the matrix A
        prob = 0

        A_lpi = lpi.get_constraints_csr().toarray()
        b_lpi = lpi.get_rhs()
        lpi_copy = LpInstance(lpi)
       
        for region in self.regions:
            # TODO: move this to a function
            A = A_lpi.copy()
            b = b_lpi.copy()
            # check if it's feasible before computing volume
            col_index = 0
            A_col_index = 0
            for (lbound, ubound) in region:
                if lbound == ubound and type(lbound) != tuple:
                    if col_index not in fixed_indices:
                        glpk.glp_set_col_bnds(lpi_copy.lp, A_col_index + 1, glpk.GLP_FX, lbound, lbound) # needs: import swiglpk as glpk
                        A_col_index += 1
                    col_index += 1
                # Handle one-hot type
                elif type(lbound) == tuple:
                    for val in lbound:
                        if col_index not in fixed_indices:
                            glpk.glp_set_col_bnds(lpi_copy.lp, A_col_index + 1, glpk.GLP_FX, val, val) # needs: import swiglpk as glpk
                            A_col_index += 1
                        col_index += 1
                else:
                    if col_index not in fixed_indices:
                        glpk.glp_set_col_bnds(lpi_copy.lp, A_col_index + 1, glpk.GLP_DB, lbound, ubound) # needs: import swiglpk as glpk
                        A_col_index += 1
                    col_index += 1

            feasible = lpi_copy.is_feasible()
            if not feasible:
                continue

            point = []
            to_eliminate_cols = []
            to_eliminate_vals = []
            to_keep_cols = []
            col_index = 0
            A_col_index = 0
            for i, (lbound, ubound) in enumerate(region):
                p = lbound if lbound == ubound else (lbound + ubound) / 2
                if lbound == ubound and type(p) != tuple:
                    if col_index not in fixed_indices:
                        to_eliminate_cols.append(A_col_index)
                        to_eliminate_vals.append(lbound)
                        A_col_index += 1
                    col_index += 1
                    point.append(p)
                elif type(p) == tuple:
                    for val in p:
                        if col_index not in fixed_indices:
                            to_eliminate_cols.append(A_col_index)
                            to_eliminate_vals.append(val)
                            A_col_index += 1
                        col_index += 1
                    point.extend(p)
                else:
                    row = np.zeros((1, A.shape[1]))
                    row[0, A_col_index] = 1
                    A = np.concatenate((A, row, -row), axis=0)
                    b = np.append(b, (ubound, -lbound))
                    if col_index not in fixed_indices:
                        to_keep_cols.append(A_col_index)
                        A_col_index += 1
                    col_index += 1
                    point.append(p)

            point = np.array(point)
            if len(to_eliminate_cols) > 0:
                to_eliminate_vals = np.array(to_eliminate_vals)
                b -= A[:, to_eliminate_cols] @ to_eliminate_vals
                A = A[:, to_keep_cols]

            A = np.array(A)
            b = np.array(b)
            poly = reduce(Polytope(A, b))
            poly.minrep = True
            #prob += volume(poly, extreme)*p
            lin_forms = self.feature_generator.reshape(-1, self.feature_generator.shape[-1])
            lin_forms = np.concatenate([lin_forms, np.ones((1, lin_forms.shape[1]))], axis=0)
            P = (np.arange(self.feature_generator.shape[0])[:, None].repeat(self.feature_generator.shape[1], axis=0).flatten() + 1)
            P = np.concatenate([P, np.zeros(1)], axis=0)

            vertices, active_constraints = dd_extreme(poly)
            v = lawrence_integrate_linear_form(poly, vertices, active_constraints, coeffs=lin_forms, P=P)
            coeffs = np.concatenate([self.coeffs.flatten(), self.bias*np.ones(1)], axis=-1)
            #prob += max(np.sum(v*coeffs), 0)

            #  TODO: do this in a nicer way
            p_cont = max(np.sum(v*coeffs)/self.continuous_volume, 0)
            p_disc = self.sample(*point)/self._continuous_sample(point[list(self.continuous_indices)])
            prob +=  p_cont*p_disc

        return prob

class ExpFitKernelPDF:
    """Fits a polynomial to the results generated by a kernel"""
    # TODO: add "given", which allows us to predicate on some variable when predicting the 
    # continuous features
    def __init__(self, X, input_class, n_generated_features=50, bandwidth=0.1, kernel="gaussian"):
        self.kernel = kernel
        # TODO: check whether we still need this assumption, and if so, ensure it is enforced
        # Assumption: One-Hot indices are contiguous in the array for a one-hot feature
        matches_given = np.apply_along_axis(lambda x: x in input_class, 1, X)
        if np.sum(matches_given) == 0:
            raise ValueError("No instance of class found.")

        X = X[matches_given]

        continuous_bounds = []
        spec = input_class.specification
        for label, index in spec.continuous_indices.items():
            lb, ub = spec.variables[label].min, spec.variables[label].max
            continuous_bounds.append((index, lb, ub))

        self.continuous_bounds = [[bound[1:]] for bound in sorted(continuous_bounds)]

        self.discrete_indices = tuple(sorted(spec.discrete_indices.values()))
        self.continuous_indices = tuple(sorted(spec.continuous_indices.values()))
        self.one_hot_indices = tuple(sorted(spec.one_hot_indices.values()))

        self.discrete = [make_discrete_distribution(X[:, i]) for i in self.discrete_indices]
        self.one_hot = [make_one_hot_distribution(X[:, index_group]) for index_group in self.one_hot_indices]

        self.discrete_bounds = [[(k, k) for k in sorted(d.keys())] for d in self.discrete]
        self.one_hot_bounds = [[(one_hot(k, len(d.keys())), one_hot(k, len(d.keys()))) for k in sorted(d.keys())] for d in self.one_hot]


        self.feature_generator = np.random.uniform(low=-1, high=1, size=(n_generated_features, len(self.continuous_indices))) \
            + 1j * np.random.uniform(low=-2*np.pi, high=2*np.pi, size=(n_generated_features, len(self.continuous_indices)))
        self.bandwidth = bandwidth
        self.coeffs, self.bias = self._fit(X)

        
        self.discrete_funcs = [make_discrete_func(d) for d in self.discrete]
        self.one_hot_funcs = [make_one_hot_func(d) for d in self.one_hot]

        # Note we can probably do this analytically
        self.continuous_volume = self._continuous_integrate([b[0] for b in self.continuous_bounds])
        self.discrete_volumes = [sum(f(k) for k in d.keys()) for f, d in zip(self.discrete_funcs, self.discrete)]
        self.one_hot_volumes = [sum(f(k) for k in d.keys()) for f, d in zip(self.one_hot_funcs, self.one_hot)]

        regions = sorted(
                chain(
                    zip([[k] for k in self.continuous_indices], self.continuous_bounds),
                    zip([[k] for k in self.discrete_indices], self.discrete_bounds),
                    zip(self.one_hot_indices, self.one_hot_bounds)
                    )

        )
        self._regions = tuple(map(lambda x: x[1], regions))

    def _fit(self, X):
        # Use the kernel to generate the "ground truth" pdf
        X = X[:, self.continuous_indices]
        self.kern = KernelDensity(kernel=self.kernel, bandwidth=self.bandwidth).fit(X)
        X_test = np.concatenate([np.random.uniform(low=b[0][0], high=b[0][1], size=(2*X.shape[0], 1)) for b in self.continuous_bounds], axis=-1)
        X_full = np.concatenate((X, X_test), axis=0)
        y = np.exp(self.kern.score_samples(X_full))

        # Now we generate the features
        X_train = np.exp(np.einsum('fi,bi->bf', self.feature_generator, X_full)).real

        lm = LinearRegression().fit(X_train, y)
        return lm.coef_, lm.intercept_

    def _continuous_sample(self, x):
        X = np.exp(np.einsum('fi,...i->...f', self.feature_generator, x)).real
        y = np.einsum('...f,f->...', X, self.coeffs) + self.bias
        return y/self.continuous_volume

    def sample(self, *args):
        """get probability density at a point"""

        x = np.array(args)
        p = self._continuous_sample(x[list(self.continuous_indices)])
        for func, volume, index in zip(self.discrete_funcs, self.discrete_volumes, self.discrete_indices):
            p *= func(x[index])/volume

        for func, volume, index_group in zip(self.one_hot_funcs, self.one_hot_volumes, self.one_hot_indices):
            p *= func(x[index_group])/volume

        return p

    @property
    def regions(self):
        return product(*self._regions)

    def _continuous_integrate(self, bounds):
        n = len(bounds)
        A = np.concatenate([np.eye(n), -np.eye(n)], axis=0)
        b = np.concatenate([np.array([b[1] for b in bounds]), -np.array([b[0] for b in bounds])], axis=0)

        poly = reduce(Polytope(A, b))
        poly.minrep = True


        vertices, active_constraints = dd_extreme(poly)
        v = self.bias * lawrence_integrate_linear_form(poly, vertices, active_constraints)[0]

        integrals = lawrence_integrate_linear_exp(poly, vertices, active_constraints, self.feature_generator).real

        return np.sum(self.coeffs * integrals) + v


    def integrate(self, lpi, fixed_indices):
        # Fixed indices are those that are fixed in the input set
        # So they are not in the matrix A
        prob = 0

        A_lpi = lpi.get_constraints_csr().toarray()
        b_lpi = lpi.get_rhs()
        lpi_copy = LpInstance(lpi)
       
        for region in self.regions:
            # TODO: move this to a function
            A = A_lpi.copy()
            b = b_lpi.copy()
            # check if it's feasible before computing volume
            col_index = 0
            A_col_index = 0
            for (lbound, ubound) in region:
                if lbound == ubound and type(lbound) != tuple:
                    if col_index not in fixed_indices:
                        glpk.glp_set_col_bnds(lpi_copy.lp, A_col_index + 1, glpk.GLP_FX, lbound, lbound) # needs: import swiglpk as glpk
                        A_col_index += 1
                    col_index += 1
                # Handle one-hot type
                elif type(lbound) == tuple:
                    for val in lbound:
                        if col_index not in fixed_indices:
                            glpk.glp_set_col_bnds(lpi_copy.lp, A_col_index + 1, glpk.GLP_FX, val, val) # needs: import swiglpk as glpk
                            A_col_index += 1
                        col_index += 1
                else:
                    if col_index not in fixed_indices:
                        glpk.glp_set_col_bnds(lpi_copy.lp, A_col_index + 1, glpk.GLP_DB, lbound, ubound) # needs: import swiglpk as glpk
                        A_col_index += 1
                    col_index += 1

            feasible = lpi_copy.is_feasible()
            if not feasible:
                continue

            point = []
            to_eliminate_cols = []
            to_eliminate_vals = []
            to_keep_cols = []
            col_index = 0
            A_col_index = 0
            for i, (lbound, ubound) in enumerate(region):
                p = lbound if lbound == ubound else (lbound + ubound) / 2
                if lbound == ubound and type(p) != tuple:
                    if col_index not in fixed_indices:
                        to_eliminate_cols.append(A_col_index)
                        to_eliminate_vals.append(lbound)
                        A_col_index += 1
                    col_index += 1
                    point.append(p)
                elif type(p) == tuple:
                    for val in p:
                        if col_index not in fixed_indices:
                            to_eliminate_cols.append(A_col_index)
                            to_eliminate_vals.append(val)
                            A_col_index += 1
                        col_index += 1
                    point.extend(p)
                else:
                    row = np.zeros((1, A.shape[1]))
                    row[0, A_col_index] = 1
                    A = np.concatenate((A, row, -row), axis=0)
                    b = np.append(b, (ubound, -lbound))
                    if col_index not in fixed_indices:
                        to_keep_cols.append(A_col_index)
                        A_col_index += 1
                    col_index += 1
                    point.append(p)

            point = np.array(point)
            if len(to_eliminate_cols) > 0:
                to_eliminate_vals = np.array(to_eliminate_vals)
                b -= A[:, to_eliminate_cols] @ to_eliminate_vals
                A = A[:, to_keep_cols]

            A = np.array(A)
            b = np.array(b)
            poly = reduce(Polytope(A, b))
            poly.minrep = True

            vertices, active_constraints = dd_extreme(poly)
            v = self.bias*lawrence_integrate_linear_form(poly, vertices, active_constraints)[0]
            integrals = lawrence_integrate_linear_exp(poly, vertices, active_constraints, self.feature_generator).real

            p_cont = max(np.sum(self.coeffs * integrals) + v, 0)/self.continuous_volume

            #  TODO: do this in a nicer way
            p_disc = self.sample(*point)/self._continuous_sample(point[list(self.continuous_indices)])
            prob +=  p_cont*p_disc

        return prob



def make_discrete_distribution(data):
    dist = defaultdict(int)

    for x in data:
        dist[x] += 1

    return dist

def make_one_hot_distribution(data):
    dist = defaultdict(int)

    for x in data:
        hot_index = np.argmax(x == 1)
        dist[hot_index] += 1

    return dist


def make_discrete_func(dist):
    def func(x):
        return dist.get(x, 0)

    return func

def make_one_hot_func(dist):
    def func(x):
        hot_index = np.argmax(x == 1)
        return dist.get(hot_index, 0)

    return func

def one_hot(hot_index, length):
    h = [0]*length
    h[hot_index] = 1
    return tuple(h)
